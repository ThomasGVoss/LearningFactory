{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNasy5G/JJ3dHDMeHWDy5uS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThomasGVoss/LearningFactory/blob/main/Lab_Data_Prep_with_Pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **CR**oss *I*ndustry *S*tandard **P**rocess for **D**ata **M**ining (CRISP-DM) is a process model that serves as the base for a data science process. It has six sequential phases:\n",
        "\n",
        "\n",
        "\n",
        "1.   Business understanding – What does the business need?\n",
        "2.   Data understanding – What data do we have / need? Is it clean?\n",
        "3.   Data preparation – How do we organize the data for modeling?\n",
        "4.   Modeling – What modeling techniques should we apply?\n",
        "5.   Evaluation – Which model best meets the business objectives?\n",
        "6.   Deployment – How do stakeholders access the results?\n",
        "\n",
        "\n",
        "Published in 1999 to standardize data mining processes across industries, it has since become the most common methodology for data mining, analytics, and data science projects."
      ],
      "metadata": {
        "id": "BmtufzPd2djq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 1: Business Understanding\n",
        "\n",
        "The Business Understanding phase focuses on understanding the objectives and requirements of the project. Aside from the third task, the three other tasks in this phase are foundational project management activities that are universal to most projects:\n",
        "\n",
        "**Determine business objectives:** You should first “thoroughly understand, from a business perspective, what the customer really wants to accomplish.” (CRISP-DM Guide) and then define business success criteria.\n",
        "\n",
        "**Assess situation:** Determine resources availability, project requirements, assess risks and contingencies, and conduct a cost-benefit analysis.\n",
        "\n",
        "**Determine data mining goals:** In addition to defining the business objectives, you should also define what success looks like from a technical data mining perspective.\n",
        "\n",
        "**Produce project plan:** Select technologies and tools and define detailed plans for each project phase.\n",
        "\n",
        "While many teams hurry through this phase, establishing a strong business understanding is like building the foundation of a house – absolutely essential.\n",
        "\n",
        "This enables close coordination between technical department of Data Scientists, Data Analysts and Data Engineers with business stakeholders."
      ],
      "metadata": {
        "id": "U6Yn2ZEL3BUV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ut5xcJw3Tac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2: Data Understanding\n",
        "\n",
        "\n",
        "Next is the Data Understanding phase. Adding to the foundation of Business Understanding, it drives the focus to identify, collect, and analyze the data sets that can help you accomplish the project goals. This phase also has four tasks:\n",
        "\n",
        "Collect initial data: Acquire the necessary data and (if necessary) load it into your analysis tool.\n",
        "Describe data: Examine the data and document its surface properties like data format, number of records, or field identities.\n",
        "Explore data: Dig deeper into the data. Query it, visualize it, and identify relationships among the data.\n",
        "Verify data quality: How clean/dirty is the data? Document any quality issues."
      ],
      "metadata": {
        "id": "QGnjIyIT3Ei_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMFYYqo75sQE"
      },
      "outputs": [],
      "source": [
        "import numpy as np                                # For matrix operations and numerical processing\n",
        "import pandas as pd                               # For munging tabular data\n",
        "import matplotlib.pyplot as plt                   # For charts and visualizations\n",
        "from IPython.display import Image                 # For displaying images in the notebook\n",
        "from IPython.display import display               # For displaying outputs in the notebook\n",
        "from time import gmtime, strftime                 # For labeling SageMaker models, endpoints, etc.\n",
        "import sys                                        # For writing outputs to notebook\n",
        "import math                                       # For ceiling function\n",
        "import json                                       # For parsing hosting outputs\n",
        "import os                                         # For manipulating filepath names\n",
        "import zipfile                                    # For unzipping\n",
        "\n",
        "# ensure graphs are displayed correctly inline in this notebook\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting and Loading the data"
      ],
      "metadata": {
        "id": "uWCThRl63u79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/ThomasGVoss/LearningFactory/main/apjournal.csv\n",
        "!wget https://raw.githubusercontent.com/ThomasGVoss/LearningFactory/main/kundenauftrag.csv\n",
        "!wget https://raw.githubusercontent.com/ThomasGVoss/LearningFactory/main/produktionsauftrag.csv\n"
      ],
      "metadata": {
        "id": "L5H87LZ_5suy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col = ['ProcessID','RoundId','Workstation','Null','Start','End']\n",
        "data = pd.read_csv('/content/apjournal.csv', header=None, names=col, index_col=0 , sep=',',on_bad_lines='skip')\n",
        "pd.set_option('display.max_columns', 500)   # Make sure we can see all of the columns\n",
        "pd.set_option('display.max_rows', 20) # Keep the output on one page\n",
        "data = data.drop(columns='Null')\n",
        "#last output of a cell is automatically displayed in this case the pandas DataFrame\n",
        "data"
      ],
      "metadata": {
        "id": "xWWYyzM87HnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look into the round we played and talk about the data. \n",
        "\n"
      ],
      "metadata": {
        "id": "H8t60yVeAwtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.loc[data['RoundId'] == 216]"
      ],
      "metadata": {
        "id": "Bdynbjk3AwIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task: \n",
        "\n",
        "Please think back on the game and match your experience from the game with the data set you have been presented with. Please generate a description of the data. What are the Rows and Collums representing, what type of values are you looking at? \n",
        "\n",
        "Below you find some examples of ways to access the data.\n"
      ],
      "metadata": {
        "id": "DaLMfYV5-vSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Start'] = pd.to_datetime(data['Start'])\n",
        "data['End'] = pd.to_datetime(data['End'])\n",
        "data.dtypes\n"
      ],
      "metadata": {
        "id": "a2-UPlUwJj7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a column\n",
        "ws = data['Workstation']\n",
        "ws"
      ],
      "metadata": {
        "id": "IdSHQZdmAS5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.sort_values(by=\"Start\")"
      ],
      "metadata": {
        "id": "bKr5Oe2MAaT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping\n",
        "data.groupby('Workstation').size()"
      ],
      "metadata": {
        "id": "NuH93T28Ac-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate the duration based on the end and the start date\n",
        "data['Duration'] = data['End'] - data['Start']\n",
        "\n",
        "# Series.dt - Accessor object for datetimelike properties of the Series values.\n",
        "data['Seconds'] = data['Duration'].dt.total_seconds()\n",
        "\n",
        "#drop the duration col \n",
        "data = data.drop(columns=['Duration'])"
      ],
      "metadata": {
        "id": "xHx1BsnCJfmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploration \n",
        " \n",
        "Let's start exploring the data. First, let's understand how the features are distributed."
      ],
      "metadata": {
        "id": "FgDC0BkZBCMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "Dfy6oOX3KCgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's find the mean value of each process step \n",
        "data.groupby(['Workstation']).mean()"
      ],
      "metadata": {
        "id": "eUfdwMnmBDY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.groupby(['ProcessID']).apply(print)"
      ],
      "metadata": {
        "id": "fcSbJZuOM6My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: \n",
        "Please take a look at the Produktionsauftrag.csv - what can you find out? "
      ],
      "metadata": {
        "id": "Zzjdv7u2Nf3I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SR4uGYDn4Q8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 3: Data Preparation \n",
        "Transformation / Feature engineering\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Cleaning up data is part of nearly every machine learning project. It takes up a lot of time and is a necessity for a good model.\n",
        "\n",
        "**Select data:** Determine which data sets will be used and document reasons for inclusion/exclusion.\n",
        "\n",
        "**Clean data:** Often this is the lengthiest task. Without it, you’ll likely fall victim to garbage-in, garbage-out. A common practice during this task is to correct, impute, or remove erroneous values.\n",
        "\n",
        "**Construct data:** Derive new attributes that will be helpful. For example, derive someone’s body mass index from height and weight fields.\n",
        "\n",
        "**Integrate data:** Create new data sets by combining data from multiple sources.\n",
        "\n",
        "**Format data:** Re-format data as necessary. For example, you might convert string values that store numbers to numeric values so that you can perform mathematical operations."
      ],
      "metadata": {
        "id": "nogxAeg9N-kI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the data can you match the ProcessID with a variant?"
      ],
      "metadata": {
        "id": "qOvFBMRi4OAm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dbfKLtLsNxBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "can you group the data based on the type of car and the workstation used? "
      ],
      "metadata": {
        "id": "h4pZzoVBOhsT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xj6Ia1gSOm9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you maybe add more steps to the data set? Such as the start and the end? "
      ],
      "metadata": {
        "id": "ihHtCKwJUbbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for each ProcessID add 2 rows to the datatable with station PPS and Storage? \n",
        "# think which time would be usefull? In which table would you find those data items? "
      ],
      "metadata": {
        "id": "wfBMVukbUb2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## End of Lab 1\n",
        "We now have gained an understanding of our data and prepared our data ...\n",
        "\n",
        "Please download the file after generation for further processing"
      ],
      "metadata": {
        "id": "98FdjJD7ON27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('output.csv')"
      ],
      "metadata": {
        "id": "jYBqEVD8ORD6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}